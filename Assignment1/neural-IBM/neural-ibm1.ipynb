{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural IBM1\n",
    "\n",
    "NLP2 \n",
    "Author: Joost Bastings https://bastings.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the paths to our training and validation data, English side\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['36', 'th', 'Parliament', ',', '2', 'nd', 'Session'], ['36', 'e', 'Législature', ',', '2', 'ième', 'Session'])\n",
      "(['edited', 'HANSARD', '*', 'NUMBER', '1'], ['hansard', 'RÉVISÉ', '*', 'NUMÉRO', '1'])\n",
      "(['contents'], ['table', 'DES', 'MATIÈRES'])\n",
      "(['Tuesday', ',', 'October', '12', ',', '1999'], ['le', 'mardi', '12', 'octobre', '1999'])\n"
     ]
    }
   ],
   "source": [
    "# check utils.py if you want to see how smart_reader and bitext_reader work in detail\n",
    "from utils import smart_reader, bitext_reader\n",
    "\n",
    "    \n",
    "def bitext_reader_demo(src_path, trg_path):\n",
    "  \"\"\"Demo of the bitext reader.\"\"\"\n",
    " \n",
    "  # create a reader\n",
    "  src_reader = smart_reader(src_path)\n",
    "  trg_reader = smart_reader(trg_path)\n",
    "  bitext = bitext_reader(src_reader, trg_reader)\n",
    "\n",
    "  # to see that it really works, try this:\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))  \n",
    "\n",
    "\n",
    "bitext_reader_demo(train_e_path, train_f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 178928 sentences with max_length = 30\n"
     ]
    }
   ],
   "source": [
    "# To see how many sentences are left if you filter by length, you can do this:\n",
    "\n",
    "def demo_number_filtered_sentence_pairs(src_path, trg_path):\n",
    "  src_reader = smart_reader(src_path)\n",
    "  trg_reader = smart_reader(trg_path)\n",
    "  max_length = 30\n",
    "  bitext = bitext_reader(src_reader, trg_reader, max_length=max_length)   \n",
    "  num_sentences = sum([1 for _ in bitext])\n",
    "  print(\"There are {} sentences with max_length = {}\".format(num_sentences, max_length))\n",
    "  \n",
    "  \n",
    "demo_number_filtered_sentence_pairs(train_e_path, train_f_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's create a vocabulary!\n",
    "\n",
    "We first define a class `Vocabulary` that helps us convert tokens (words) into numbers. This is useful later, because then we can e.g. index a word embedding table using the ID of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check vocabulary.py to see how the Vocabulary class is defined\n",
    "from vocabulary import OrderedCounter, Vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our Vocabulary class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 36640\n",
      "Trimmed vocabulary size: 1005\n",
      "The index of \"<PAD>\" is: 0\n",
      "The index of \"<UNK>\" is: 1\n",
      "The index of \"the\" is: 5\n",
      "The token with index 0 is: <PAD>\n",
      "The token with index 1 is: <UNK>\n",
      "The token with index 2 is: <S>\n",
      "The token with index 3 is: </S>\n",
      "The token with index 4 is: <NULL>\n",
      "The token with index 5 is: the\n",
      "The token with index 6 is: .\n",
      "The token with index 7 is: ,\n",
      "The token with index 8 is: of\n",
      "The token with index 9 is: to\n",
      "The index of \"!@!_not_in_vocab_!@!\" is: 1\n"
     ]
    }
   ],
   "source": [
    "def vocabulary_demo():\n",
    "\n",
    "  # We used up a few lines in the previous example, so we set up\n",
    "  # our data generator again.\n",
    "  corpus = smart_reader(train_e_path)    \n",
    "\n",
    "  # Let's create a vocabulary given our (tokenized) corpus\n",
    "  vocabulary = Vocabulary(corpus=corpus)\n",
    "  print(\"Original vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "  # Now we only keep the highest-frequency words\n",
    "  vocabulary_size=1000\n",
    "  vocabulary.trim(vocabulary_size)\n",
    "  print(\"Trimmed vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "  # Now we can get word indexes using v.get_word_id():\n",
    "  for t in [\"<PAD>\", \"<UNK>\", \"the\"]:\n",
    "    print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "\n",
    "  # And the inverse too, using v.i2t:\n",
    "  for i in range(10):\n",
    "    print(\"The token with index {} is: {}\".format(i, vocabulary.get_token(i)))\n",
    "\n",
    "  # Now let's try to get a word ID for a word not in the vocabulary\n",
    "  # we should get 1 (so, <UNK>)\n",
    "  for t in [\"!@!_not_in_vocab_!@!\"]:\n",
    "    print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "    \n",
    "    \n",
    "vocabulary_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the vocabularies that we use further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 1005\n",
      "French vocabulary size: 1005\n",
      "\n",
      "A few English words:\n",
      "questions\n",
      "justice\n",
      "solution\n",
      "Indian\n",
      "following\n",
      "\n",
      "A few French words:\n",
      "o\n",
      "application\n",
      "manque\n",
      "prochain\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Using only 1000 words will result in many UNKs, but\n",
    "# it will make training a lot faster. \n",
    "# If you have a fast computer, a GPU, or a lot of time,\n",
    "# try with 10000 instead.\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)    \n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "print(\"English vocabulary size: {}\".format(len(vocabulary_e)))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)    \n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))\n",
    "print(\"French vocabulary size: {}\".format(len(vocabulary_f)))\n",
    "print()\n",
    "\n",
    "\n",
    "def sample_words(vocabulary, n=5):\n",
    "  \"\"\"Print a few words from the vocabulary.\"\"\"\n",
    "  for _ in range(n):\n",
    "    token_id = np.random.randint(0, len(vocabulary) - 1)\n",
    "    print(vocabulary.get_token(token_id))\n",
    "\n",
    "\n",
    "print(\"A few English words:\")\n",
    "sample_words(vocabulary_e, n=5)\n",
    "print()\n",
    "\n",
    "print(\"A few French words:\")\n",
    "sample_words(vocabulary_f, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batching\n",
    "\n",
    "With our vocabulary, we still need a method that converts a whole sentence to a sequence of IDs.\n",
    "And, to speed up training, we would like to get a so-called mini-batch at a time: multiple of such sequences together. So our function takes a corpus iterator and a vocabulary, and returns a mini-batch of shape [Batch, Time], where the first dimension indexes the sentences in the batch, and the second the time steps in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import iterate_minibatches, prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch of data that we will train on, as tokens:\n",
      "[(['36', 'th', 'Parliament', ',', '2', 'nd', 'Session'],\n",
      "  ['36', 'e', 'Législature', ',', '2', 'ième', 'Session']),\n",
      " (['edited', 'HANSARD', '*', 'NUMBER', '1'],\n",
      "  ['hansard', 'RÉVISÉ', '*', 'NUMÉRO', '1']),\n",
      " (['contents'], ['table', 'DES', 'MATIÈRES']),\n",
      " (['Tuesday', ',', 'October', '12', ',', '1999'],\n",
      "  ['le', 'mardi', '12', 'octobre', '1999'])]\n",
      "\n",
      "These are our inputs (i.e. words replaced by IDs):\n",
      "[[  4   1 745 325   7 262   1   1]\n",
      " [  4   1   1  67   1 238   0   0]\n",
      " [  4   1   0   0   0   0   0   0]\n",
      " [  4   1   7 813 882   7 297   0]]\n",
      "\n",
      "These are the outputs (the foreign sentences):\n",
      "[[  1   1   1   7 254   1   1]\n",
      " [  1   1  62   1 250   0   0]\n",
      " [  1 463   1   0   0   0   0]\n",
      " [  6   1   1 840 295   0   0]]\n",
      "\n",
      "This is the batch of data that we will train on, as tokens:\n",
      "[(['opening',\n",
      "   'OF',\n",
      "   'THE',\n",
      "   'SECOND',\n",
      "   'SESSION',\n",
      "   'OF',\n",
      "   'THE',\n",
      "   '36',\n",
      "   'TH',\n",
      "   'PARLIAMENT'],\n",
      "  ['ouverture',\n",
      "   'DE',\n",
      "   'LA',\n",
      "   'DEUXIÈME',\n",
      "   'SESSION',\n",
      "   'DE',\n",
      "   'LA',\n",
      "   '36E',\n",
      "   'LÉGISLATURE']),\n",
      " (['oaths', 'OF', 'OFFICE'], ['les', 'SERMENTS', 'De', 'OFFICE']),\n",
      " (['bill', 'C', '-', '1', '.'], ['projet', 'de', 'loi', 'C', '-', '1', '.']),\n",
      " (['introduction', 'and', 'first', 'reading'],\n",
      "  ['présentation', 'et', 'première', 'lecture'])]\n",
      "\n",
      "These are our inputs (i.e. words replaced by IDs):\n",
      "[[  4   1 488 800   1   1 488 800   1   1   1]\n",
      " [  4   1 488   1   0   0   0   0   0   0   0]\n",
      " [  4  63  99  20 238   6   0   0   0   0   0]\n",
      " [  4 931  10 125 385   0   0   0   0   0   0]]\n",
      "\n",
      "These are the outputs (the foreign sentences):\n",
      "[[  1 239 542   1   1 239 542   1   1]\n",
      " [  9   1   1   1   0   0   0   0   0]\n",
      " [ 40   5  34  94  14 250   8   0   0]\n",
      " [832  13 227 336   0   0   0   0   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_reader = smart_reader(train_e_path)\n",
    "trg_reader = smart_reader(train_f_path)\n",
    "bitext = bitext_reader(src_reader, trg_reader)\n",
    "\n",
    "\n",
    "for batch_id, batch in enumerate(iterate_minibatches(bitext, batch_size=4)):\n",
    "\n",
    "  print(\"This is the batch of data that we will train on, as tokens:\")\n",
    "  pprint(batch)\n",
    "  print()\n",
    "\n",
    "  x, y = prepare_data(batch, vocabulary_e, vocabulary_f)\n",
    "\n",
    "  print(\"These are our inputs (i.e. words replaced by IDs):\")\n",
    "  print(x)\n",
    "  print()\n",
    "  \n",
    "  print(\"These are the outputs (the foreign sentences):\")\n",
    "  print(y)\n",
    "  print()\n",
    "\n",
    "  if batch_id > 0:\n",
    "    break  # stop after the first batch, this is just a demonstration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice the following:\n",
    "\n",
    "1. Every English sequence starts with a 4, the ID for < NULL \\>.\n",
    "2. The longest sequence in the batch contains no padding symbols. Any sequences shorter, however, will have padding zeros.\n",
    "\n",
    "With our input pipeline in place, now let's create a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check neuralibm1.py for the Model code\n",
    "# Implement Neural IBM 1 on this class\n",
    "from neuralibm1 import NeuralIBM1Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Now that we have a model, we need to train it. To do so we define a Trainer class that takes our model as an argument and trains it, keeping track of some important information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check neuralibm1trainer.py for the Trainer code\n",
    "from neuralibm1trainer import NeuralIBM1Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a model and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_embedded (?, ?, 64)\n",
      "longy2 Tensor(\"strided_slice_2:0\", shape=(), dtype=int32)\n",
      "Tensor(\"transpose_1:0\", shape=(?, ?), dtype=float32)\n",
      "pa_x (?, 1, ?)\n",
      "Tensor(\"Tile:0\", shape=(?, ?, ?), dtype=float32)\n",
      "(?, 64)\n",
      "lalalala\n",
      "mlp (?, 64)\n",
      "w (64, 128)\n",
      "bias (128,)\n",
      "Tensor(\"Add_1:0\", shape=(?, 1005), dtype=float32)\n",
      "(?, ?, ?) (?, ?, 1005)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, ?), dtype=int64)\n",
      "longy Tensor(\"strided_slice_2:0\", shape=(), dtype=int32)\n",
      "shapes 16 Tensor(\"strided_slice_2:0\", shape=(), dtype=int32) (?,) (?, 1005)\n",
      "Training with B=16 max_length=30 lr=0.001 lr_decay=0.0\n",
      "Initializing variables..\n",
      "Training star§ted..\n",
      "Shuffling training data\n",
      "Iter   100 loss 342.989227 accuracy 0.06 lr 0.001000\n",
      "Iter   200 loss 209.796783 accuracy 0.00 lr 0.001000\n",
      "Iter   300 loss 232.055634 accuracy 0.02 lr 0.001000\n",
      "Iter   400 loss 183.062988 accuracy 0.03 lr 0.001000\n",
      "Iter   500 loss 81.848068 accuracy 0.05 lr 0.001000\n",
      "Iter   600 loss 103.780548 accuracy 0.04 lr 0.001000\n",
      "Iter   700 loss 122.925041 accuracy 0.02 lr 0.001000\n",
      "Iter   800 loss 81.499039 accuracy 0.04 lr 0.001000\n",
      "Iter   900 loss 52.054161 accuracy 0.05 lr 0.001000\n",
      "Iter  1000 loss 55.509724 accuracy 0.05 lr 0.001000\n",
      "Iter  1100 loss 43.155064 accuracy 0.01 lr 0.001000\n",
      "Iter  1200 loss 47.839771 accuracy 0.04 lr 0.001000\n",
      "Iter  1300 loss 64.984161 accuracy 0.03 lr 0.001000\n",
      "Iter  1400 loss 45.068073 accuracy 0.04 lr 0.001000\n",
      "Iter  1500 loss 36.728321 accuracy 0.04 lr 0.001000\n",
      "Iter  1600 loss 35.133789 accuracy 0.06 lr 0.001000\n",
      "Iter  1700 loss 36.383308 accuracy 0.01 lr 0.001000\n",
      "Iter  1800 loss 57.529247 accuracy 0.01 lr 0.001000\n",
      "Iter  1900 loss 26.677076 accuracy 0.08 lr 0.001000\n",
      "Iter  2000 loss 26.353643 accuracy 0.05 lr 0.001000\n",
      "Iter  2100 loss 31.758781 accuracy 0.07 lr 0.001000\n",
      "Iter  2200 loss 30.829199 accuracy 0.10 lr 0.001000\n",
      "Iter  2300 loss 15.149780 accuracy 0.03 lr 0.001000\n",
      "Iter  2400 loss 32.455414 accuracy 0.10 lr 0.001000\n",
      "Iter  2500 loss 14.555034 accuracy 0.01 lr 0.001000\n",
      "Iter  2600 loss 25.906687 accuracy 0.07 lr 0.001000\n",
      "Iter  2700 loss 18.257029 accuracy 0.03 lr 0.001000\n",
      "Iter  2800 loss 21.062098 accuracy 0.08 lr 0.001000\n",
      "Iter  2900 loss 15.735268 accuracy 0.03 lr 0.001000\n",
      "Iter  3000 loss 24.900450 accuracy 0.04 lr 0.001000\n",
      "Iter  3100 loss 15.126732 accuracy 0.03 lr 0.001000\n",
      "Iter  3200 loss 19.568190 accuracy 0.02 lr 0.001000\n",
      "Iter  3300 loss 15.146968 accuracy 0.04 lr 0.001000\n",
      "Iter  3400 loss 14.363161 accuracy 0.05 lr 0.001000\n",
      "Iter  3500 loss 21.446699 accuracy 0.03 lr 0.001000\n",
      "Iter  3600 loss 15.531057 accuracy 0.06 lr 0.001000\n",
      "Iter  3700 loss 13.813888 accuracy 0.00 lr 0.001000\n",
      "Iter  3800 loss 11.779510 accuracy 0.03 lr 0.001000\n",
      "Iter  3900 loss 15.679414 accuracy 0.05 lr 0.001000\n",
      "Iter  4000 loss 11.473589 accuracy 0.02 lr 0.001000\n",
      "Iter  4100 loss 9.136822 accuracy 0.01 lr 0.001000\n",
      "Iter  4200 loss 18.241688 accuracy 0.09 lr 0.001000\n",
      "Iter  4300 loss 10.521020 accuracy 0.03 lr 0.001000\n",
      "Iter  4400 loss 11.307740 accuracy 0.07 lr 0.001000\n",
      "Iter  4500 loss 12.986782 accuracy 0.09 lr 0.001000\n",
      "Iter  4600 loss 9.304247 accuracy 0.09 lr 0.001000\n",
      "Iter  4700 loss 7.745497 accuracy 0.03 lr 0.001000\n",
      "Iter  4800 loss 7.196676 accuracy 0.06 lr 0.001000\n",
      "Iter  4900 loss 11.832775 accuracy 0.11 lr 0.001000\n",
      "Iter  5000 loss 6.309873 accuracy 0.01 lr 0.001000\n",
      "Iter  5100 loss 8.627921 accuracy 0.08 lr 0.001000\n",
      "Iter  5200 loss 6.486702 accuracy 0.06 lr 0.001000\n",
      "Iter  5300 loss 8.860181 accuracy 0.02 lr 0.001000\n",
      "Iter  5400 loss 6.480056 accuracy 0.04 lr 0.001000\n",
      "Iter  5500 loss 6.751554 accuracy 0.04 lr 0.001000\n",
      "Iter  5600 loss 7.636697 accuracy 0.06 lr 0.001000\n",
      "Iter  5700 loss 9.297119 accuracy 0.05 lr 0.001000\n",
      "Iter  5800 loss 6.754580 accuracy 0.02 lr 0.001000\n",
      "Iter  5900 loss 6.437344 accuracy 0.07 lr 0.001000\n",
      "Iter  6000 loss 5.866683 accuracy 0.01 lr 0.001000\n",
      "Iter  6100 loss 5.061719 accuracy 0.01 lr 0.001000\n",
      "Iter  6200 loss 6.886431 accuracy 0.04 lr 0.001000\n",
      "Iter  6300 loss 4.585770 accuracy 0.06 lr 0.001000\n",
      "Iter  6400 loss 7.140713 accuracy 0.10 lr 0.001000\n",
      "Iter  6500 loss 5.192306 accuracy 0.06 lr 0.001000\n",
      "Iter  6600 loss 6.135262 accuracy 0.03 lr 0.001000\n",
      "Iter  6700 loss 3.819746 accuracy 0.10 lr 0.001000\n",
      "Iter  6800 loss 7.470051 accuracy 0.08 lr 0.001000\n",
      "Iter  6900 loss 4.514711 accuracy 0.03 lr 0.001000\n",
      "Iter  7000 loss 4.988265 accuracy 0.10 lr 0.001000\n",
      "Iter  7100 loss 4.028954 accuracy 0.01 lr 0.001000\n",
      "Iter  7200 loss 5.545173 accuracy 0.00 lr 0.001000\n",
      "Iter  7300 loss 5.332362 accuracy 0.16 lr 0.001000\n",
      "Iter  7400 loss 6.505553 accuracy 0.03 lr 0.001000\n",
      "Iter  7500 loss 4.110745 accuracy 0.02 lr 0.001000\n",
      "Iter  7600 loss 4.021022 accuracy 0.01 lr 0.001000\n",
      "Iter  7700 loss 5.171012 accuracy 0.00 lr 0.001000\n",
      "Iter  7800 loss 3.454533 accuracy 0.07 lr 0.001000\n",
      "Iter  7900 loss 4.469052 accuracy 0.01 lr 0.001000\n",
      "Iter  8000 loss 4.407633 accuracy 0.10 lr 0.001000\n",
      "Iter  8100 loss 4.010804 accuracy 0.03 lr 0.001000\n",
      "Iter  8200 loss 3.894906 accuracy 0.06 lr 0.001000\n",
      "Iter  8300 loss 3.832207 accuracy 0.05 lr 0.001000\n",
      "Iter  8400 loss 4.459646 accuracy 0.02 lr 0.001000\n",
      "Iter  8500 loss 4.019576 accuracy 0.11 lr 0.001000\n",
      "Iter  8600 loss 4.129014 accuracy 0.10 lr 0.001000\n",
      "Iter  8700 loss 3.176030 accuracy 0.05 lr 0.001000\n",
      "Iter  8800 loss 3.970616 accuracy 0.10 lr 0.001000\n",
      "Iter  8900 loss 3.225512 accuracy 0.03 lr 0.001000\n",
      "Iter  9000 loss 4.460628 accuracy 0.01 lr 0.001000\n",
      "Iter  9100 loss 3.706783 accuracy 0.02 lr 0.001000\n",
      "Iter  9200 loss 3.284062 accuracy 0.01 lr 0.001000\n",
      "Iter  9300 loss 2.738804 accuracy 0.11 lr 0.001000\n",
      "Iter  9400 loss 4.445323 accuracy 0.09 lr 0.001000\n",
      "Iter  9500 loss 3.437465 accuracy 0.05 lr 0.001000\n",
      "Iter  9600 loss 4.310534 accuracy 0.01 lr 0.001000\n",
      "Iter  9700 loss 2.908723 accuracy 0.03 lr 0.001000\n",
      "Iter  9800 loss 3.887580 accuracy 0.10 lr 0.001000\n",
      "Iter  9900 loss 3.298977 accuracy 0.01 lr 0.001000\n",
      "Iter 10000 loss 3.222071 accuracy 0.03 lr 0.001000\n",
      "Iter 10100 loss 2.996597 accuracy 0.09 lr 0.001000\n",
      "Iter 10200 loss 2.870586 accuracy 0.03 lr 0.001000\n",
      "Iter 10300 loss 3.945366 accuracy 0.07 lr 0.001000\n",
      "Iter 10400 loss 3.617217 accuracy 0.04 lr 0.001000\n",
      "Iter 10500 loss 3.276852 accuracy 0.03 lr 0.001000\n",
      "Iter 10600 loss 3.434736 accuracy 0.06 lr 0.001000\n",
      "Iter 10700 loss 3.465206 accuracy 0.05 lr 0.001000\n",
      "Iter 10800 loss 4.081648 accuracy 0.02 lr 0.001000\n",
      "Iter 10900 loss 4.413536 accuracy 0.17 lr 0.001000\n",
      "Iter 11000 loss 3.879547 accuracy 0.03 lr 0.001000\n",
      "Iter 11100 loss 3.096751 accuracy 0.11 lr 0.001000\n",
      "Epoch 1 loss 28.905382 accuracy 0.04 val_aer 0.99 val_acc 0.13\n",
      "Model saved in file: ./model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 3.165565 accuracy 0.02 lr 0.001000\n",
      "Iter   200 loss 3.142142 accuracy 0.12 lr 0.001000\n",
      "Iter   300 loss 3.096452 accuracy 0.12 lr 0.001000\n",
      "Iter   400 loss 2.744009 accuracy 0.03 lr 0.001000\n",
      "Iter   500 loss 2.837764 accuracy 0.01 lr 0.001000\n",
      "Iter   600 loss 2.497840 accuracy 0.07 lr 0.001000\n",
      "Iter   700 loss 5.036511 accuracy 0.15 lr 0.001000\n",
      "Iter   800 loss 2.591356 accuracy 0.17 lr 0.001000\n",
      "Iter   900 loss 3.653846 accuracy 0.04 lr 0.001000\n",
      "Iter  1000 loss 3.299137 accuracy 0.10 lr 0.001000\n",
      "Iter  1100 loss 3.559403 accuracy 0.03 lr 0.001000\n",
      "Iter  1200 loss 3.681051 accuracy 0.13 lr 0.001000\n",
      "Iter  1300 loss 1.486489 accuracy 0.09 lr 0.001000\n",
      "Iter  1400 loss 2.383570 accuracy 0.11 lr 0.001000\n",
      "Iter  1500 loss 4.019916 accuracy 0.03 lr 0.001000\n",
      "Iter  1600 loss 2.552160 accuracy 0.04 lr 0.001000\n",
      "Iter  1700 loss 2.565816 accuracy 0.12 lr 0.001000\n",
      "Iter  1800 loss 4.446250 accuracy 0.03 lr 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cbf50493a0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m# now we can start training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training star§ted..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/natural_language_processing_2/Assignment1/neural-IBM/neuralibm1trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         }\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "  # some hyper-parameters\n",
    "  # tweak them as you wish\n",
    "  batch_size=16  # on CPU, use something much smaller e.g. 1-16\n",
    "  max_length=30\n",
    "  lr = 0.001\n",
    "  lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "  emb_dim = 64\n",
    "  mlp_dim = 128\n",
    "  \n",
    "  # our model\n",
    "  model = NeuralIBM1Model(\n",
    "    x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "    batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "  \n",
    "  # our trainer\n",
    "  trainer = NeuralIBM1Trainer(\n",
    "    model, train_e_path, train_f_path, \n",
    "    dev_e_path, dev_f_path, dev_wa,\n",
    "    num_epochs=10, batch_size=batch_size, \n",
    "    max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "  # now first TF needs to initialize all the variables\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # now we can start training!\n",
    "  print(\"Training star§ted..\")\n",
    "  trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
